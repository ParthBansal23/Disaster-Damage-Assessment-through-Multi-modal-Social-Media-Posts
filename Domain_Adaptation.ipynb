{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/BXT/subset/"
      ],
      "metadata": {
        "id": "iQVywA-XCqZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbfbf11-8646-46a2-9522-778277e0f9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/BXT/subset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "xbAdhaLXsrbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV Files Path and Parameters\n",
        "source_path = \"source_subset.csv\"\n",
        "target_path = \"target_subset.csv\"\n",
        "batch_size = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ODVOfx1u-Gr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SourceTargetDataset(Dataset):\n",
        "    def __init__(self, source_csv_path, target_csv_path):\n",
        "\n",
        "        # Load source data (features and labels) with explicit dtypes\n",
        "        feature_dtypes = {f'dim_{i}': np.float32 for i in range(1024)}\n",
        "        feature_dtypes['label'] = np.float32  # Add label dtype\n",
        "\n",
        "        # Load source data with explicit type conversion\n",
        "        self.source_df = pd.read_csv(source_csv_path)\n",
        "\n",
        "        # Load target data (features only)\n",
        "        self.target_df = pd.read_csv(target_csv_path)\n",
        "\n",
        "        # Extract feature column names (dim_0 to dim_1023)\n",
        "        self.feature_cols = [f'dim_{i}' for i in range(1024)]\n",
        "\n",
        "        # Check if all feature columns exist in both dataframes\n",
        "        assert all(col in self.source_df.columns for col in self.feature_cols), \"Source CSV missing some feature columns\"\n",
        "        assert all(col in self.target_df.columns for col in self.feature_cols), \"Target CSV missing some feature columns\"\n",
        "\n",
        "        # Check if label column exists in source dataframe\n",
        "        assert 'label' in self.source_df.columns, \"Source CSV missing label column\"\n",
        "\n",
        "        # Explicitly convert feature columns to float32\n",
        "        for col in self.feature_cols:\n",
        "            self.source_df[col] = self.source_df[col].astype(np.float32)\n",
        "            self.target_df[col] = self.target_df[col].astype(np.float32)\n",
        "\n",
        "        # Convert label column to float32\n",
        "        self.source_df['label'] = self.source_df['label'].astype(np.float32)\n",
        "\n",
        "        # Get dataset length (use minimum length if they differ)\n",
        "        self.length = min(len(self.source_df), len(self.target_df))\n",
        "\n",
        "        print(f\"Loaded {self.length} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get source features (shape: 1024)\n",
        "        source_features = torch.tensor(\n",
        "            self.source_df.iloc[idx][self.feature_cols].values.astype(np.float32),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Get source label\n",
        "        source_label = torch.tensor(\n",
        "            self.source_df.iloc[idx]['label'].astype(np.float32),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Get target features (shape: 1024)\n",
        "        target_features = torch.tensor(\n",
        "            self.target_df.iloc[idx][self.feature_cols].values.astype(np.float32),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'source_features': source_features,  # Shape: 1024\n",
        "            'source_label': source_label,        # Shape: 1\n",
        "            'target_features': target_features   # Shape: 1024\n",
        "\n",
        "            }\n",
        "\n",
        "def get_data_loaders(source_csv_path, target_csv_path, batch_size=32, shuffle=True, num_workers=2):\n",
        "\n",
        "    dataset = SourceTargetDataset(source_csv_path, target_csv_path)\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return data_loader\n"
      ],
      "metadata": {
        "id": "XVM6x23eszvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 768),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1)"
      ],
      "metadata": {
        "id": "lFoqumZnu6xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DomainAdaptation():\n",
        "\n",
        "    def __init__(self, source_csv_path, target_csv_path, batch_size=16):\n",
        "\n",
        "        self.dataloader = get_data_loaders(source_path, target_path, batch_size=batch_size)\n",
        "\n",
        "        self.G = FeatureExtractor(input_size=1024).to(device)               # Generator\n",
        "        self.C = Classifier(input_size=512).to(device)                      # Classifier\n",
        "        self.C1 = Classifier(input_size=512).to(device)                     # Classifier-1\n",
        "        self.C2 = Classifier(input_size=512).to(device)                     # Classifier-2\n",
        "        self.D = Discriminator(input_size=512).to(device)                   # Discriminator\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = 1e-3\n",
        "        self.epochs = 20\n",
        "\n",
        "        self.set_optimizer()                # Setting Up Adam Optimizer\n",
        "        self.reset_grad()\n",
        "\n",
        "        # Saving Pseudo-Labels of Target Domain Batch\n",
        "        self.output_cr_t_C_label = np.zeros(self.batch_size)\n",
        "\n",
        "        self.checkpoint_dir = 'sample_data'            # Directory Path where the weights are being saved\n",
        "\n",
        "        # Create checkpoint directory if it doesn't exist\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    def set_optimizer(self):\n",
        "        self.opt_g = optim.Adam(self.G.parameters(), lr=self.lr, weight_decay=0.0005)\n",
        "        self.opt_c = optim.Adam(self.C.parameters(), lr=self.lr, weight_decay=0.0005)\n",
        "        self.opt_c1 = optim.Adam(self.C1.parameters(), lr=self.lr, weight_decay=0.0005)\n",
        "        self.opt_c2 = optim.Adam(self.C2.parameters(), lr=self.lr, weight_decay=0.0005)\n",
        "        self.opt_d = optim.Adam(self.D.parameters(), lr=self.lr, weight_decay=0.0005)\n",
        "\n",
        "\n",
        "    def reset_grad(self):\n",
        "        self.opt_g.zero_grad()\n",
        "        self.opt_c.zero_grad()\n",
        "        self.opt_c1.zero_grad()\n",
        "        self.opt_c2.zero_grad()\n",
        "        self.opt_d.zero_grad()\n",
        "\n",
        "\n",
        "    def discrepancy(self, out1, out2):\n",
        "        # Ensure tensors have shape [batch_size, num_classes] for softmax\n",
        "        out1_reshaped = out1.view(-1, 1)\n",
        "        out2_reshaped = out2.view(-1, 1)\n",
        "        # Concatenate with 1-values to create 2-class outputs for softmax\n",
        "        out1_2class = torch.cat([1-out1_reshaped, out1_reshaped], dim=1)\n",
        "        out2_2class = torch.cat([1-out2_reshaped, out2_reshaped], dim=1)\n",
        "        return torch.mean(torch.abs(F.softmax(out1_2class, dim=1) - F.softmax(out2_2class, dim=1)))\n",
        "\n",
        "\n",
        "    def linear_mmd(self, f_of_X, f_of_Y):\n",
        "        loss = 0.0\n",
        "        delta = f_of_X - f_of_Y\n",
        "        loss = torch.mean(torch.mm(delta, torch.transpose(delta, 0, 1)))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def ent(self, output):\n",
        "        # Ensure output has shape [batch_size, num_classes] for softmax\n",
        "        output_reshaped = output.view(-1, 1)\n",
        "        # Concatenate with 1-values to create 2-class outputs for softmax\n",
        "        output_2class = torch.cat([1-output_reshaped, output_reshaped], dim=1)\n",
        "        return torch.mean(F.softmax(output_2class + 1e-6, dim=1) * torch.log(F.softmax(output_2class + 1e-6, dim=1))).negative()\n",
        "\n",
        "\n",
        "    def save_weights(self, epoch):\n",
        "\n",
        "        epoch_dir = os.path.join(self.checkpoint_dir, f'epoch_{epoch}')\n",
        "        os.makedirs(epoch_dir, exist_ok=True)\n",
        "\n",
        "        # Save each model's state dict\n",
        "        torch.save(self.G.state_dict(), os.path.join(epoch_dir, 'feature_extractor.pth'))\n",
        "        torch.save(self.C.state_dict(), os.path.join(epoch_dir, 'classifier.pth'))\n",
        "        torch.save(self.C1.state_dict(), os.path.join(epoch_dir, 'classifier1.pth'))\n",
        "        torch.save(self.C2.state_dict(), os.path.join(epoch_dir, 'classifier2.pth'))\n",
        "        torch.save(self.D.state_dict(), os.path.join(epoch_dir, 'discriminator.pth'))\n",
        "\n",
        "        # Save optimizer states\n",
        "        torch.save(self.opt_g.state_dict(), os.path.join(epoch_dir, 'opt_g.pth'))\n",
        "        torch.save(self.opt_c.state_dict(), os.path.join(epoch_dir, 'opt_c.pth'))\n",
        "        torch.save(self.opt_c1.state_dict(), os.path.join(epoch_dir, 'opt_c1.pth'))\n",
        "        torch.save(self.opt_c2.state_dict(), os.path.join(epoch_dir, 'opt_c2.pth'))\n",
        "        torch.save(self.opt_d.state_dict(), os.path.join(epoch_dir, 'opt_d.pth'))\n",
        "\n",
        "        # Save a single checkpoint with all models (for convenience)\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'G_state_dict': self.G.state_dict(),\n",
        "            'C_state_dict': self.C.state_dict(),\n",
        "            'C1_state_dict': self.C1.state_dict(),\n",
        "            'C2_state_dict': self.C2.state_dict(),\n",
        "            'D_state_dict': self.D.state_dict(),\n",
        "            'opt_g_state_dict': self.opt_g.state_dict(),\n",
        "            'opt_c_state_dict': self.opt_c.state_dict(),\n",
        "            'opt_c1_state_dict': self.opt_c1.state_dict(),\n",
        "            'opt_c2_state_dict': self.opt_c2.state_dict(),\n",
        "            'opt_d_state_dict': self.opt_d.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
        "\n",
        "        print(f\"Model weights saved for epoch {epoch}\")\n",
        "\n",
        "\n",
        "    def load_checkpoint(self, epoch):\n",
        "\n",
        "        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "            self.G.load_state_dict(checkpoint['G_state_dict'])\n",
        "            self.C.load_state_dict(checkpoint['C_state_dict'])\n",
        "            self.C1.load_state_dict(checkpoint['C1_state_dict'])\n",
        "            self.C2.load_state_dict(checkpoint['C2_state_dict'])\n",
        "            self.D.load_state_dict(checkpoint['D_state_dict'])\n",
        "\n",
        "            self.opt_g.load_state_dict(checkpoint['opt_g_state_dict'])\n",
        "            self.opt_c.load_state_dict(checkpoint['opt_c_state_dict'])\n",
        "            self.opt_c1.load_state_dict(checkpoint['opt_c1_state_dict'])\n",
        "            self.opt_c2.load_state_dict(checkpoint['opt_c2_state_dict'])\n",
        "            self.opt_d.load_state_dict(checkpoint['opt_d_state_dict'])\n",
        "\n",
        "            print(f\"Loaded checkpoint from epoch {epoch}\")\n",
        "            return epoch\n",
        "        else:\n",
        "            print(f\"No checkpoint found for epoch {epoch}\")\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        A_st_min = 0\n",
        "        A_st_max = 1\n",
        "        min_J_w = 0\n",
        "        max_J_w = 1\n",
        "        A_st_norm = 0.5\n",
        "        J_w_norm = 0.5\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.G.train()\n",
        "            self.C.train()\n",
        "            self.C1.train()\n",
        "            self.C2.train()\n",
        "            self.D.train()\n",
        "            fea_for_LDA = np.empty(shape=(0, 512))\n",
        "            fea_s_for_LDA = np.empty(shape=(0, 512))\n",
        "            label_for_LDA = np.empty(shape=(0, 1))\n",
        "            label_s_for_LDA = []\n",
        "\n",
        "            # Training loop\n",
        "            for batch_idx, batch in enumerate(tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{self.epochs}\")):\n",
        "                # Access source features (shape: B x 1024)\n",
        "                source_features = batch['source_features']\n",
        "\n",
        "                # Access source labels (shape: B)\n",
        "                source_labels = batch['source_label']\n",
        "\n",
        "                # Access target features (shape: B x 1024)\n",
        "                target_features = batch['target_features']\n",
        "\n",
        "                # Ensure correct batch size (handle last batch which might be smaller)\n",
        "                actual_batch_size = source_features.size(0)\n",
        "                if actual_batch_size != self.batch_size:\n",
        "                    # Resize output_cr_t_C_label if needed\n",
        "                    if len(self.output_cr_t_C_label) != actual_batch_size:\n",
        "                        self.output_cr_t_C_label = np.zeros(actual_batch_size)\n",
        "\n",
        "                # Sending to GPU if available\n",
        "                source_features = source_features.to(device)\n",
        "                source_labels = source_labels.to(device)\n",
        "                target_features = target_features.to(device)\n",
        "\n",
        "                self.reset_grad()\n",
        "\n",
        "                # Computing T\n",
        "                T_complex = A_st_norm /(A_st_norm + (1.0 - J_w_norm))\n",
        "                T = T_complex.real\n",
        "\n",
        "                # Normal Supervised Learning on Source Domain Batch\n",
        "                for i in range(8):\n",
        "                    feat_cr_s = self.G(source_features)\n",
        "                    output_cr_s_C = self.C(feat_cr_s)\n",
        "                    loss_1 = criterion(output_cr_s_C, source_labels)\n",
        "\n",
        "                    loss_1.backward()\n",
        "                    self.opt_g.step()               # Update G\n",
        "                    self.opt_c.step()               # Update C\n",
        "                    self.reset_grad()\n",
        "\n",
        "                # Transferability\n",
        "                for i in range(1):\n",
        "                    feat_cr_s = self.G(source_features)\n",
        "                    feat_cr_t = self.G(target_features)\n",
        "\n",
        "                    # Training Discriminator of GAN\n",
        "                    output_cr_s_D = self.D(feat_cr_s)\n",
        "                    output_cr_t_D = self.D(feat_cr_t)\n",
        "                    loss_2 = criterion(output_cr_s_D, output_cr_t_D.detach())\n",
        "                    loss_2 = 0.1 * loss_2\n",
        "                    loss_2.backward()\n",
        "                    self.opt_d.step()               # Update D\n",
        "                    self.reset_grad()\n",
        "\n",
        "                    # Training All Classifiers\n",
        "                    feat_cr_s = self.G(source_features)  # Recompute features since graph was freed\n",
        "                    feat_cr_t = self.G(target_features)  # Recompute features since graph was freed\n",
        "\n",
        "                    output_cr_s_C = self.C(feat_cr_s)\n",
        "                    output_cr_t_C = self.C(feat_cr_t)\n",
        "                    output_cr_s_C1 = self.C1(feat_cr_s)\n",
        "                    output_cr_s_C2 = self.C2(feat_cr_s)\n",
        "                    output_cr_t_C1 = self.C1(feat_cr_t)\n",
        "                    output_cr_t_C2 = self.C2(feat_cr_t)\n",
        "\n",
        "                    source_labels_reshaped = source_labels.view(-1, 1)\n",
        "                    loss_cr_s = criterion(output_cr_s_C1, source_labels) + criterion(output_cr_s_C2, source_labels) + criterion(output_cr_s_C, source_labels)\n",
        "                    loss_dis1_t = self.discrepancy(output_cr_t_C1, output_cr_t_C2).negative()       # Negative\n",
        "                    loss_3 = loss_cr_s + loss_dis1_t\n",
        "                    loss_3.backward()\n",
        "\n",
        "                    self.opt_c1.step()                # Update all 3 classifiers\n",
        "                    self.opt_c2.step()\n",
        "                    self.opt_c.step()\n",
        "                    self.reset_grad()\n",
        "\n",
        "                # Balance of transferability and discriminability\n",
        "                for i in range(8):\n",
        "                    feat_cr_s = self.G(source_features)\n",
        "                    feat_cr_t = self.G(target_features)\n",
        "\n",
        "                    output_cr_s_D = self.D(feat_cr_s)\n",
        "                    output_cr_t_D = self.D(feat_cr_t)\n",
        "                    loss_4 = criterion(output_cr_s_D, output_cr_t_D.detach()).negative()           # Negative\n",
        "                    loss_4 = 0.2 * loss_4\n",
        "\n",
        "                    # Recompute target features for classifier outputs\n",
        "                    feat_cr_t = self.G(target_features)\n",
        "                    output_cr_t_C = self.C(feat_cr_t)\n",
        "                    output_cr_t_C1 = self.C1(feat_cr_t)\n",
        "                    output_cr_t_C2 = self.C2(feat_cr_t)\n",
        "\n",
        "                    # Calculating class discrimination loss (L_cd)\n",
        "                    loss_51 = self.discrepancy(output_cr_t_C1, output_cr_t_C2)\n",
        "                    loss_52 = self.discrepancy(output_cr_t_C, output_cr_t_C1)\n",
        "                    loss_53 = self.discrepancy(output_cr_t_C, output_cr_t_C2)\n",
        "                    loss_5 = loss_51 + loss_52 + loss_53\n",
        "\n",
        "                    loss_all = T*loss_4 + (1.0-T)*loss_5\n",
        "                    loss_all.backward()\n",
        "                    self.opt_g.step()           # Update G\n",
        "                    self.reset_grad()\n",
        "\n",
        "                # Re-weighting based on the uncertainty of pseudo-label\n",
        "                for i in range(1):\n",
        "                    feat_cr_t = self.G(target_features)\n",
        "                    output_cr_t_C = self.C(feat_cr_t)\n",
        "                    output_cr_t_C_de = output_cr_t_C.detach()\n",
        "\n",
        "                    # Update output_cr_t_C_label with correct batch size handling\n",
        "                    for ii in range(actual_batch_size):\n",
        "                        self.output_cr_t_C_label[ii] = output_cr_t_C_de[ii].item()\n",
        "\n",
        "                    output_cr_t_C_labels = torch.from_numpy(self.output_cr_t_C_label[:actual_batch_size]).to(device).float()\n",
        "                    Ly_ce_t = criterion(output_cr_t_C, output_cr_t_C_labels)\n",
        "\n",
        "                    # Ensure ent method is correctly defined and uses dim=1 for softmax\n",
        "                    H_emp = self.ent(output_cr_t_C)\n",
        "                    mu = (torch.exp(-H_emp)-1.0/2)/(1-1.0/2)\n",
        "                    Ly_loss = 2*(mu*Ly_ce_t+(1-mu)*H_emp)\n",
        "\n",
        "                    Ly_loss.backward()\n",
        "                    self.opt_g.step()               # Update G\n",
        "                    self.opt_c.step()               # Update C\n",
        "                    self.reset_grad()\n",
        "\n",
        "                # Data for A_st(MMD) and J_w\n",
        "                with torch.no_grad():  # No need to track gradients for these computations\n",
        "                    # for source\n",
        "                    feat_cr_s = self.G(source_features)\n",
        "                    feat_cr_t = self.G(target_features)\n",
        "                    label_predi = self.C(feat_cr_t)           # Pseudo-Labels for Target Domain Batch\n",
        "\n",
        "                    # Convert to numpy for later calculations\n",
        "                    feat_s_test_np = feat_cr_s.cpu().detach().numpy()\n",
        "                    label_s_test_np = source_labels.cpu().detach().numpy()\n",
        "\n",
        "                    # Append to our collected data\n",
        "                    label_s_for_LDA = np.append(label_s_for_LDA, label_s_test_np.flatten())\n",
        "                    fea_s_for_LDA = np.vstack((fea_s_for_LDA, feat_s_test_np)) if fea_s_for_LDA.size > 0 else feat_s_test_np\n",
        "\n",
        "                    # for target\n",
        "                    feat_test_np = feat_cr_t.cpu().detach().numpy()\n",
        "                    fea_for_LDA = np.vstack((fea_for_LDA, feat_test_np)) if fea_for_LDA.size > 0 else feat_test_np\n",
        "\n",
        "                    # Pseudo-Labels for Target Domain\n",
        "                    label_t = (label_predi >= 0.5).float()  # Binary classification threshold\n",
        "                    label_test_np = label_t.cpu().detach().numpy()\n",
        "                    label_test_np = label_test_np.reshape(-1, 1)  # Reshape to ensure correct dimensions\n",
        "\n",
        "                    label_for_LDA = np.vstack((label_for_LDA, label_test_np)) if label_for_LDA.size > 0 else label_test_np\n",
        "\n",
        "            # Calculate source domain accuracy after each epoch\n",
        "            source_accuracy = self.calculate_source_accuracy(self.G, self.C)\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs} - Source Domain Accuracy: {source_accuracy:.2f}%\")\n",
        "\n",
        "            # Save model weights after each epoch\n",
        "            self.save_weights(epoch+1)\n",
        "\n",
        "            try:\n",
        "                # MMD with Norm (A_st_norm)\n",
        "                f_of_X = torch.from_numpy(fea_s_for_LDA).float().to(device)\n",
        "                f_of_Y = torch.from_numpy(fea_for_LDA).float().to(device)\n",
        "                loss_mmd = self.linear_mmd(f_of_X, f_of_Y)\n",
        "                A_st = loss_mmd.cpu().detach().numpy()\n",
        "                A_st_max = max(abs(A_st_max), abs(A_st))\n",
        "                A_st_min = min(abs(A_st_min), abs(A_st))\n",
        "                A_st_norm = abs(A_st-A_st_min)/(A_st_max-A_st_min+1e-6)\n",
        "\n",
        "                # J_w calculation\n",
        "                self.class_num = 2  # Binary classification has 2 classes\n",
        "\n",
        "                # J_w_s with Norm\n",
        "                n_dim = 1\n",
        "                clusters1 = [0, 1]\n",
        "                label_s_for_LDA = np.array(label_s_for_LDA).reshape(-1)\n",
        "\n",
        "                Sw1 = np.zeros((fea_s_for_LDA.shape[1], fea_s_for_LDA.shape[1]))\n",
        "                for i in clusters1:\n",
        "                    class_indices = (label_s_for_LDA == i)\n",
        "                    if np.sum(class_indices) > 1:  # Ensure we have at least 2 samples for this class\n",
        "                        datai1 = fea_s_for_LDA[class_indices]\n",
        "                        datai1 = datai1 - datai1.mean(0)\n",
        "                        Swi1 = np.asmatrix(datai1).T * np.asmatrix(datai1)\n",
        "                        Sw1 += Swi1\n",
        "\n",
        "                # Between-class scatter matrix\n",
        "                SB1 = np.zeros((fea_s_for_LDA.shape[1], fea_s_for_LDA.shape[1]))\n",
        "                u1 = fea_s_for_LDA.mean(0)  # Average of all samples\n",
        "                for i in clusters1:\n",
        "                    class_indices = (label_s_for_LDA == i)\n",
        "                    if np.sum(class_indices) > 0:  # Ensure we have samples for this class\n",
        "                        Ni1 = fea_s_for_LDA[class_indices].shape[0]\n",
        "                        ui1 = fea_s_for_LDA[class_indices].mean(0)  # Average of a category\n",
        "                        SBi1 = Ni1 * np.asmatrix(ui1 - u1).T * np.asmatrix(ui1 - u1)\n",
        "                        SB1 += SBi1\n",
        "\n",
        "                # Add small regularization to ensure invertibility\n",
        "                S1 = np.linalg.inv(Sw1 + (1e-6 * np.eye(Sw1.shape[0]))) * SB1\n",
        "                eigVals1, eigVects1 = np.linalg.eig(S1)  # Find eigenvalues, eigenvectors\n",
        "                eigValInd1 = np.argsort(eigVals1)\n",
        "                eigValInd1 = eigValInd1[:(-n_dim-1):-1]\n",
        "                J_max1 = 0\n",
        "                for i in range(min(n_dim, len(eigValInd1))):\n",
        "                    J_max1 = J_max1 + np.real(eigVals1[eigValInd1[i]])  # Ensure we use real part\n",
        "\n",
        "                J_w_s = J_max1/self.class_num\n",
        "                max_J_w = max(max_J_w, J_w_s)\n",
        "                min_J_w = min(min_J_w, J_w_s)\n",
        "\n",
        "                # J_w_t calculation\n",
        "                n_dim = 1\n",
        "                label_for_LDA_flat = label_for_LDA.reshape(-1)\n",
        "                clusters = np.unique(label_for_LDA_flat)\n",
        "\n",
        "                Sw = np.zeros((fea_for_LDA.shape[1], fea_for_LDA.shape[1]))\n",
        "                for i in clusters:\n",
        "                    class_indices = (label_for_LDA_flat == i)\n",
        "                    if np.sum(class_indices) > 1:  # Ensure we have at least 2 samples for this class\n",
        "                        datai = fea_for_LDA[class_indices]\n",
        "                        datai = datai - datai.mean(0)\n",
        "                        Swi = np.asmatrix(datai).T * np.asmatrix(datai)\n",
        "                        Sw += Swi\n",
        "\n",
        "                # Between-class scatter matrix\n",
        "                SB = np.zeros((fea_for_LDA.shape[1], fea_for_LDA.shape[1]))\n",
        "                u = fea_for_LDA.mean(0)  # Average of all samples\n",
        "                for i in clusters:\n",
        "                    class_indices = (label_for_LDA_flat == i)\n",
        "                    if np.sum(class_indices) > 0:  # Ensure we have samples for this class\n",
        "                        Ni = fea_for_LDA[class_indices].shape[0]\n",
        "                        ui = fea_for_LDA[class_indices].mean(0)  # Average of a category\n",
        "                        SBi = Ni * np.asmatrix(ui - u).T * np.asmatrix(ui - u)\n",
        "                        SB += SBi\n",
        "\n",
        "                # Add small regularization to ensure invertibility\n",
        "                S = np.linalg.inv(Sw + (1e-6 * np.eye(Sw.shape[0]))) * SB\n",
        "                eigVals, eigVects = np.linalg.eig(S)  # Find eigenvalues, eigenvectors\n",
        "                eigValInd = np.argsort(eigVals)\n",
        "                eigValInd = eigValInd[:(-n_dim-1):-1]\n",
        "                J_max = 0\n",
        "                for i in range(min(n_dim, len(eigValInd))):\n",
        "                    J_max = J_max + np.real(eigVals[eigValInd[i]])  # Ensure we use real part\n",
        "\n",
        "                J_w_t = J_max/self.class_num\n",
        "                min_J_w = min(min_J_w, J_w_t)\n",
        "                max_J_w = max(max_J_w, J_w_t)\n",
        "\n",
        "                # J_w_s_norm\n",
        "                J_w = min(J_w_s, J_w_t)\n",
        "                J_w_norm = (J_w - min_J_w)/(max_J_w-min_J_w+1e-6)\n",
        "\n",
        "                print(f\"Epoch {epoch+1} - A_st_norm: {A_st_norm:.4f}, J_w_norm: {J_w_norm:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not calculate A_st_norm and J_w_norm for epoch {epoch+1}. Error: {e}\")\n",
        "                # Keep previous values\n",
        "                print(f\"Using previous values: A_st_norm: {A_st_norm:.4f}, J_w_norm: {J_w_norm:.4f}\")\n",
        "\n",
        "    def calculate_source_accuracy(self, model_G, model_C):\n",
        "        \"\"\"\n",
        "        Calculate accuracy on the source domain\n",
        "\n",
        "        Args:\n",
        "            model_G: Feature extractor model\n",
        "            model_C: Classifier model\n",
        "            dataloader: DataLoader containing source and target data\n",
        "            device: Device to run inference on (cpu or cuda)\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy on source domain\n",
        "        \"\"\"\n",
        "        model_G.eval()\n",
        "        model_C.eval()\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.dataloader:\n",
        "                source_features = batch['source_features'].to(device)\n",
        "                source_labels = batch['source_label'].to(device)\n",
        "\n",
        "                # Extract features and classify\n",
        "                features = model_G(source_features)\n",
        "                outputs = model_C(features)\n",
        "\n",
        "                # Convert sigmoid outputs to binary predictions (threshold at 0.5)\n",
        "                predicted = (outputs >= 0.5).float()\n",
        "\n",
        "                # Update statistics\n",
        "                total += source_labels.size(0)\n",
        "                correct += (predicted == source_labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Reset models back to training mode\n",
        "        model_G.train()\n",
        "        model_C.train()\n",
        "\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "4yNhZzsF7dQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DomainAdaptation(source_path, target_path, batch_size=16)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIVPqqw_Zosp",
        "outputId": "7e2e106e-1b82-43a0-f44d-5261d96c3542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1024 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 64/64 [00:10<00:00,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Source Domain Accuracy: 80.66%\n",
            "Model weights saved for epoch 1\n",
            "Epoch 1 - A_st_norm: 0.3043, J_w_norm: 0.8697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 64/64 [00:09<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Source Domain Accuracy: 75.29%\n",
            "Model weights saved for epoch 2\n",
            "Epoch 2 - A_st_norm: 1.0000, J_w_norm: 0.1244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 64/64 [00:10<00:00,  6.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Source Domain Accuracy: 80.76%\n",
            "Model weights saved for epoch 3\n",
            "Epoch 3 - A_st_norm: 1.0000, J_w_norm: 0.1327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 64/64 [00:10<00:00,  5.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Source Domain Accuracy: 82.71%\n",
            "Model weights saved for epoch 4\n",
            "Epoch 4 - A_st_norm: 1.0000, J_w_norm: 0.1043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 64/64 [00:10<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Source Domain Accuracy: 87.40%\n",
            "Model weights saved for epoch 5\n",
            "Epoch 5 - A_st_norm: 0.7715, J_w_norm: 0.6392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 64/64 [00:10<00:00,  5.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 - Source Domain Accuracy: 85.35%\n",
            "Model weights saved for epoch 6\n",
            "Epoch 6 - A_st_norm: 0.3971, J_w_norm: 0.0462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 64/64 [00:09<00:00,  6.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 - Source Domain Accuracy: 91.11%\n",
            "Model weights saved for epoch 7\n",
            "Epoch 7 - A_st_norm: 0.2861, J_w_norm: 0.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 64/64 [00:10<00:00,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 - Source Domain Accuracy: 92.68%\n",
            "Model weights saved for epoch 8\n",
            "Epoch 8 - A_st_norm: 0.6625, J_w_norm: 0.0380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 64/64 [00:11<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 - Source Domain Accuracy: 96.29%\n",
            "Model weights saved for epoch 9\n",
            "Epoch 9 - A_st_norm: 0.2286, J_w_norm: 0.1223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 64/64 [00:11<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 - Source Domain Accuracy: 87.89%\n",
            "Model weights saved for epoch 10\n",
            "Epoch 10 - A_st_norm: 0.1332, J_w_norm: 0.1768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 64/64 [00:11<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 - Source Domain Accuracy: 96.68%\n",
            "Model weights saved for epoch 11\n",
            "Epoch 11 - A_st_norm: 0.1132, J_w_norm: 0.0805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 64/64 [00:09<00:00,  6.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 - Source Domain Accuracy: 95.90%\n",
            "Model weights saved for epoch 12\n",
            "Epoch 12 - A_st_norm: 0.0358, J_w_norm: 0.0339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 64/64 [00:10<00:00,  6.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Source Domain Accuracy: 92.97%\n",
            "Model weights saved for epoch 13\n",
            "Epoch 13 - A_st_norm: 0.1419, J_w_norm: 0.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 64/64 [00:11<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 - Source Domain Accuracy: 98.05%\n",
            "Model weights saved for epoch 14\n",
            "Epoch 14 - A_st_norm: 0.1735, J_w_norm: 0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 64/64 [00:11<00:00,  5.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 - Source Domain Accuracy: 96.78%\n",
            "Model weights saved for epoch 15\n",
            "Epoch 15 - A_st_norm: 0.0480, J_w_norm: 0.0352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 64/64 [00:11<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 - Source Domain Accuracy: 91.11%\n",
            "Model weights saved for epoch 16\n",
            "Epoch 16 - A_st_norm: 0.0324, J_w_norm: 0.0233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 64/64 [00:09<00:00,  6.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 - Source Domain Accuracy: 98.14%\n",
            "Model weights saved for epoch 17\n",
            "Epoch 17 - A_st_norm: 0.0770, J_w_norm: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 64/64 [00:10<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 - Source Domain Accuracy: 89.75%\n",
            "Model weights saved for epoch 18\n",
            "Epoch 18 - A_st_norm: 0.0222, J_w_norm: 0.0197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 64/64 [00:11<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 - Source Domain Accuracy: 96.19%\n",
            "Model weights saved for epoch 19\n",
            "Epoch 19 - A_st_norm: 0.0356, J_w_norm: 0.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 64/64 [00:10<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 - Source Domain Accuracy: 97.75%\n",
            "Model weights saved for epoch 20\n",
            "Epoch 20 - A_st_norm: 0.0387, J_w_norm: 0.0228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_csv(model, checkpoint_epoch, input_csv_path, output_csv_path):\n",
        "    \"\"\"\n",
        "    Loads the saved weights for self.G and self.C from a given checkpoint epoch,\n",
        "    performs classification on an input CSV file, and saves the predictions to a new CSV.\n",
        "\n",
        "    Args:\n",
        "        model (DomainAdaptation): Instance of the DomainAdaptation class.\n",
        "        checkpoint_epoch (int): The epoch number from which to load the checkpoint.\n",
        "        input_csv_path (str): Path to the CSV file containing the features (dim_0 to dim_1023).\n",
        "        output_csv_path (str): Path to save the new CSV file with predictions.\n",
        "    \"\"\"\n",
        "    # Load the checkpoint (this updates self.G and self.C)\n",
        "    model.load_checkpoint(checkpoint_epoch)\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    model.G.eval()\n",
        "    model.C.eval()\n",
        "\n",
        "    # Read CSV file containing features. We expect columns \"dim_0\" ... \"dim_1023\".\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    feature_cols = [f\"dim_{i}\" for i in range(1024)]\n",
        "\n",
        "    # Check for missing columns\n",
        "    missing_cols = [col for col in feature_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Input CSV is missing the following columns: {missing_cols}\")\n",
        "\n",
        "    # Convert feature data to a torch tensor\n",
        "    features = torch.tensor(df[feature_cols].values.astype(np.float32))\n",
        "    features = features.to(device)  # 'device' is defined earlier in your code\n",
        "\n",
        "    # Perform forward pass without tracking gradients\n",
        "    with torch.no_grad():\n",
        "        # Extract features using the feature extractor\n",
        "        extracted_features = model.G(features)\n",
        "        # Get classifier outputs using the classifier network\n",
        "        outputs = model.C(extracted_features)\n",
        "        # Convert outputs to binary predictions using a 0.5 threshold\n",
        "        predictions = (outputs >= 0.5).float().cpu().numpy()\n",
        "\n",
        "    # Append predictions to the DataFrame and save to a new CSV file\n",
        "    df[\"prediction\"] = predictions\n",
        "    columns_to_drop = [f\"dim_{i}\" for i in range(1024)]\n",
        "\n",
        "    # Drop the columns (only those that exist in the DataFrame)\n",
        "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Predictions saved in: {output_csv_path}\")\n",
        "\n",
        "\n",
        "# Then, after training and saving weights, classify new data as follows:\n",
        "classify_csv(model, checkpoint_epoch=20, input_csv_path=\"/content/dev_merge.csv\", output_csv_path=\"predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-ZcWMXFkNqj",
        "outputId": "75689f09-b8aa-4e18-9212-bf797546b48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from epoch 20\n",
            "Predictions saved in: predictions.csv\n"
          ]
        }
      ]
    }
  ]
}